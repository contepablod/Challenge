{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "import torch\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "\n",
    "# Utils\n",
    "import neptune\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from IPython.display import display, clear_output\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "!set 'PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__OS DETAILS: posix.uname_result(sysname='Linux', nodename='pdconte-localhost', release='6.10.6-2-liquorix-amd64', version='#1 ZEN SMP PREEMPT liquorix 6.10-6ubuntu1~jammy (2024-08-20)', machine='x86_64')\n",
      "__Python VERSION: 3.12.3 (main, Sep  4 2024, 12:08:24) [GCC 13.2.0]\n",
      "__pyTorch VERSION: 2.4.1+cu124\n",
      "__CUDA VERSION:\n",
      "Wed Sep 11 19:07:17 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1650 Ti     Off |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   64C    P5              9W /   50W |      94MiB /   4096MiB |     40%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     10248      G   /usr/lib/xorg/Xorg                             91MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "__CUDNN VERSION: 90100\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Wed_Aug_14_10:10:22_PDT_2024\n",
      "Cuda compilation tools, release 12.6, V12.6.68\n",
      "Build cuda_12.6.r12.6/compiler.34714021_0\n",
      "__Number CUDA Devices: 1\n",
      "__Devices\n",
      "Active CUDA Device: GPU 0\n",
      "Available devices  1\n",
      "Current cuda device  0\n"
     ]
    }
   ],
   "source": [
    "print(\"__OS DETAILS:\", os.uname())\n",
    "print(\"__Python VERSION:\", sys.version)\n",
    "print(\"__pyTorch VERSION:\", torch.__version__)\n",
    "print(\"__CUDA VERSION:\")\n",
    "! nvidia-smi\n",
    "print(\"__CUDNN VERSION:\", torch.backends.cudnn.version())\n",
    "!nvcc --version\n",
    "print(\"__Number CUDA Devices:\", torch.cuda.device_count())\n",
    "print(\"__Devices\")\n",
    "print(\"Active CUDA Device: GPU\", torch.cuda.current_device())\n",
    "print(\"Available devices \", torch.cuda.device_count())\n",
    "print(\"Current cuda device \", torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Params:\n",
    "    images_dir: str = \"./X_train_corrected/images\"\n",
    "    labels_csv: str = \"./y_train_PlZf4rH.csv\"\n",
    "    test_images_dir: str = \"./X_test_Xqv4sJa/images\"\n",
    "    image_shape: tuple = (36, 36)\n",
    "    image_reshape: tuple = (32, 32)\n",
    "    batch_size: int = 128\n",
    "    true_boolean: bool = True\n",
    "    false_boolean: bool = False\n",
    "    num_workers: int = 0\n",
    "    channels: int = 1\n",
    "    classes: int = 1\n",
    "    encoder_name: str = \"resnet152\"\n",
    "    encoder_weights: str = \"imagenet\"\n",
    "    decoder: str = \"Yes\"\n",
    "    decoder_attention_type: str | None = \"scse\" if decoder == \"Yes\" else None\n",
    "    activation: str = \"No\"\n",
    "    activation_layer: str | None = \"sigmoid\" if activation == \"Yes\" else None\n",
    "    compile_mode: str = \"default\"\n",
    "    num_epochs: int = 5\n",
    "    learning_rate: float = 1e-3\n",
    "    max_grad_norm: float = 1.0\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the well ID from the image name\n",
    "def get_well_id(img_name):\n",
    "    return int(img_name.split(\"_\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WellDataset(Dataset):\n",
    "    def __init__(self, transform=None, exclude_wells=None):\n",
    "        if exclude_wells is None:\n",
    "            exclude_wells = []\n",
    "        self.images_dir = Params.images_dir\n",
    "        self.labels_df = pd.read_csv(Params.labels_csv, index_col=0)\n",
    "\n",
    "        # Filter out specific wells\n",
    "        self.labels_df = self.labels_df.loc[\n",
    "            [\n",
    "                img_name\n",
    "                for img_name in self.labels_df.index\n",
    "                if get_well_id(img_name) not in exclude_wells\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.labels_df.index[idx]\n",
    "        image_path = f\"{self.images_dir}/{img_name}.npy\"\n",
    "        image = np.load(image_path)\n",
    "        label = (\n",
    "            self.labels_df.loc[img_name]\n",
    "            .values.reshape(Params.image_shape)\n",
    "            .astype(\"float32\")\n",
    "        )\n",
    "\n",
    "        # Check for NaNs and replace them with zeros\n",
    "        image = np.nan_to_num(image)\n",
    "        label = np.nan_to_num(label)\n",
    "\n",
    "        image = torch.tensor(image, dtype=torch.float32).unsqueeze(0)\n",
    "        label = torch.tensor(label, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            label = self.transform(label)\n",
    "\n",
    "        return img_name, image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(Params.image_reshape),\n",
    "        # transforms.Normalize(mean=[0.485], std=[0.229]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def calculate_mean_std(dataset):\n",
    "    # Accumulators for sum and sum of squares\n",
    "    mean_sum = 0\n",
    "    mean_squared_sum = 0\n",
    "    total_pixels = 0\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=Params.batch_size,\n",
    "        shuffle=Params.false_boolean,\n",
    "        num_workers=Params.num_workers,\n",
    "        pin_memory=Params.true_boolean,\n",
    "    )  # Load the dataset in batches to avoid memory issues\n",
    "\n",
    "    for _, images, _ in loader:\n",
    "        # Assuming the images are in shape (batch_size, channels, height, width)\n",
    "        batch_pixels = (\n",
    "            images.numel()\n",
    "        )  # Total number of pixels in the batch (batch_size * channels * height * width)\n",
    "        total_pixels += batch_pixels\n",
    "\n",
    "        # Sum of all pixel values in the batch\n",
    "        mean_sum += images.sum()\n",
    "\n",
    "        # Sum of all squared pixel values in the batch\n",
    "        mean_squared_sum += (images**2).sum()\n",
    "\n",
    "    # Calculate the overall mean\n",
    "    mean = mean_sum / total_pixels\n",
    "\n",
    "    # Calculate variance (E[X^2] - E[X]^2) and then standard deviation\n",
    "    variance = (mean_squared_sum / total_pixels) - mean**2\n",
    "    std = torch.sqrt(variance)\n",
    "\n",
    "    return mean.item(), std.item()\n",
    "\n",
    "\n",
    "# Create the dataset\n",
    "dataset = WellDataset(transform=transform)  # Load dataset without normalization\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "mean, std = calculate_mean_std(dataset)\n",
    "\n",
    "print(f\"Mean: {mean}, Std: {std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well counts: Counter({13: 1905, 3: 1774, 7: 1654, 11: 1512, 6: 964, 2: 616, 14: 355, 15: 203, 1: 166, 9: 166, 5: 98, 10: 80, 8: 78, 4: 59, 12: 44})\n"
     ]
    }
   ],
   "source": [
    "# # Define wells for training and validation\n",
    "# train_wells = [1, 2, 4, 5, 8, 9, 12, 13, 14] #,15]\n",
    "# val_wells = [3, 6, 7, 10, 11]\n",
    "\n",
    "# Load the dataset\n",
    "dataset = WellDataset(transform=transform)\n",
    "\n",
    "# Get well IDs for all images\n",
    "well_ids = [get_well_id(img_name) for img_name in dataset.labels_df.index]\n",
    "\n",
    "# Use Counter to check patch distribution across wells\n",
    "well_counts = Counter(well_ids)\n",
    "print(\"Well counts:\", well_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract well IDs and their corresponding counts\n",
    "wells = list(well_counts.keys())\n",
    "counts = list(well_counts.values())\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(wells, counts, color=\"skyblue\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Well ID\")\n",
    "plt.ylabel(\"Patch Count\")\n",
    "plt.title(\"Patch Count per Well\")\n",
    "plt.xticks(wells)  # Show well IDs as x-axis ticks\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 7576\n",
      "Validation set size: 1895\n"
     ]
    }
   ],
   "source": [
    "# Convert well IDs to numpy array for StratifiedKFold\n",
    "well_ids = [well_id for well_id in well_ids if well_id != 15]\n",
    "well_ids = np.array(well_ids)\n",
    "# Perform stratified split with desired proportions\n",
    "stratified_split = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# Get the first split for train and validation sets\n",
    "for train_idx, val_idx in stratified_split.split(np.zeros(len(well_ids)), well_ids):\n",
    "    break\n",
    "\n",
    "# Create the train and validation datasets\n",
    "train_dataset = Subset(WellDataset(transform=transform, exclude_wells=[15]), train_idx)\n",
    "val_dataset = Subset(WellDataset(transform=transform, exclude_wells=[15]), val_idx)\n",
    "\n",
    "# Output the sizes of the datasets\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=Params.batch_size,\n",
    "    shuffle=Params.true_boolean,\n",
    "    num_workers=Params.num_workers,\n",
    "    pin_memory=Params.true_boolean,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=Params.batch_size,\n",
    "    shuffle=Params.false_boolean,\n",
    "    num_workers=Params.num_workers,\n",
    "    pin_memory=Params.true_boolean,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:08<00:00,  7.41it/s]\n",
      "100%|██████████| 15/15 [00:01<00:00,  9.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common pixel values in training: [(np.float32(-999.2499), 12116), (np.float32(-999.24994), 11405), (np.float32(-998.9999), 6197), (np.float32(-998.99994), 3186), (np.float32(-999.0), 1488), (np.float32(-0.01751561), 1122), (np.float32(-999.25), 574), (np.float32(-0.017515609), 467), (np.float32(-0.0031377221), 158), (np.float32(-0.012903972), 158), (np.float32(-0.0019819508), 141), (np.float32(-0.007561739), 136), (np.float32(-0.0027884839), 135), (np.float32(-0.010151794), 135), (np.float32(-0.011802452), 135), (np.float32(-0.0043994337), 135), (np.float32(-0.0019005684), 131), (np.float32(0.00043340618), 130), (np.float32(0.01014927), 130), (np.float32(0.0021405995), 119)]\n",
      "Most common pixel values in validation: [(np.float32(-999.2499), 2213), (np.float32(-999.24994), 2068), (np.float32(-998.9999), 580), (np.float32(-998.99994), 301), (np.float32(-999.0), 144), (np.float32(-999.25), 108), (np.float32(-8.940696e-08), 25), (np.float32(0.0080257505), 25), (np.float32(-0.008765539), 24), (np.float32(-0.0003552049), 24), (np.float32(0.0073045543), 24), (np.float32(-0.004297235), 23), (np.float32(-0.0029562558), 23), (np.float32(-0.0025139893), 23), (np.float32(0.020517897), 23), (np.float32(-0.0028772203), 21), (np.float32(0.015787464), 21), (np.float32(0.00024034372), 20), (np.float32(-0.011164447), 20), (np.float32(-0.0067320163), 20)]\n",
      "Default values detected: [-998.9999389648438, -999.0, -999.25, -999.2498779296875, -998.9998779296875, -999.2499389648438]\n"
     ]
    }
   ],
   "source": [
    "train_value_counter = Counter()\n",
    "val_value_counter = Counter()\n",
    "\n",
    "# Loop through the train DataLoader to extract pixel values\n",
    "for _, images, _ in tqdm(train_loader):\n",
    "    # Flatten the images to create a 1D array of all pixel values\n",
    "    flattened_images = images.cpu().numpy().flatten()\n",
    "    train_value_counter.update(flattened_images)\n",
    "\n",
    "# Loop through the val DataLoader to extract pixel values\n",
    "for _, images, _ in tqdm(val_loader):\n",
    "    # Flatten the images to create a 1D array of all pixel values\n",
    "    flattened_images = images.cpu().numpy().flatten()\n",
    "    val_value_counter.update(flattened_images)\n",
    "\n",
    "# Display the most common pixel values for train and val\n",
    "train_most_common_values = train_value_counter.most_common(20)\n",
    "val_most_common_values = val_value_counter.most_common(20)\n",
    "\n",
    "print(\"Most common pixel values in training:\", train_most_common_values)\n",
    "print(\"Most common pixel values in validation:\", val_most_common_values)\n",
    "\n",
    "# Function to determine if a value is a default value based on criteria\n",
    "def is_default_value(value, threshold=-998.0):\n",
    "    return value <= threshold\n",
    "\n",
    "\n",
    "# Extract default values from train and val common values\n",
    "train_default_values = [\n",
    "    float(val) for val, count in train_most_common_values if is_default_value(val)\n",
    "]\n",
    "val_default_values = [\n",
    "    float(val) for val, count in val_most_common_values if is_default_value(val)\n",
    "]\n",
    "\n",
    "# Merge both lists and get unique default values\n",
    "default_values = list(set(train_default_values + val_default_values))\n",
    "\n",
    "print(\"Default values detected:\", default_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unet(\n",
       "  (encoder): ResNetEncoder(\n",
       "    (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (6): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (7): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (6): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (7): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (8): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (9): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (10): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (11): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (12): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (13): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (14): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (15): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (16): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (17): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (18): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (19): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (20): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (21): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (22): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (23): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (24): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (25): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (26): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (27): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (28): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (29): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (30): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (31): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (32): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (33): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (34): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (35): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): UnetDecoder(\n",
       "    (center): Identity()\n",
       "    (blocks): ModuleList(\n",
       "      (0): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(3072, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): SCSEModule(\n",
       "            (cSE): Sequential(\n",
       "              (0): AdaptiveAvgPool2d(output_size=1)\n",
       "              (1): Conv2d(3072, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (2): ReLU(inplace=True)\n",
       "              (3): Conv2d(192, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (4): Sigmoid()\n",
       "            )\n",
       "            (sSE): Sequential(\n",
       "              (0): Conv2d(3072, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): SCSEModule(\n",
       "            (cSE): Sequential(\n",
       "              (0): AdaptiveAvgPool2d(output_size=1)\n",
       "              (1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (2): ReLU(inplace=True)\n",
       "              (3): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (4): Sigmoid()\n",
       "            )\n",
       "            (sSE): Sequential(\n",
       "              (0): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(768, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): SCSEModule(\n",
       "            (cSE): Sequential(\n",
       "              (0): AdaptiveAvgPool2d(output_size=1)\n",
       "              (1): Conv2d(768, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (2): ReLU(inplace=True)\n",
       "              (3): Conv2d(48, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (4): Sigmoid()\n",
       "            )\n",
       "            (sSE): Sequential(\n",
       "              (0): Conv2d(768, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): SCSEModule(\n",
       "            (cSE): Sequential(\n",
       "              (0): AdaptiveAvgPool2d(output_size=1)\n",
       "              (1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (2): ReLU(inplace=True)\n",
       "              (3): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (4): Sigmoid()\n",
       "            )\n",
       "            (sSE): Sequential(\n",
       "              (0): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): SCSEModule(\n",
       "            (cSE): Sequential(\n",
       "              (0): AdaptiveAvgPool2d(output_size=1)\n",
       "              (1): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (2): ReLU(inplace=True)\n",
       "              (3): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (4): Sigmoid()\n",
       "            )\n",
       "            (sSE): Sequential(\n",
       "              (0): Conv2d(384, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): SCSEModule(\n",
       "            (cSE): Sequential(\n",
       "              (0): AdaptiveAvgPool2d(output_size=1)\n",
       "              (1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (2): ReLU(inplace=True)\n",
       "              (3): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (4): Sigmoid()\n",
       "            )\n",
       "            (sSE): Sequential(\n",
       "              (0): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): SCSEModule(\n",
       "            (cSE): Sequential(\n",
       "              (0): AdaptiveAvgPool2d(output_size=1)\n",
       "              (1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (2): ReLU(inplace=True)\n",
       "              (3): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (4): Sigmoid()\n",
       "            )\n",
       "            (sSE): Sequential(\n",
       "              (0): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): SCSEModule(\n",
       "            (cSE): Sequential(\n",
       "              (0): AdaptiveAvgPool2d(output_size=1)\n",
       "              (1): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (2): ReLU(inplace=True)\n",
       "              (3): Conv2d(2, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (4): Sigmoid()\n",
       "            )\n",
       "            (sSE): Sequential(\n",
       "              (0): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): SCSEModule(\n",
       "            (cSE): Sequential(\n",
       "              (0): AdaptiveAvgPool2d(output_size=1)\n",
       "              (1): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (2): ReLU(inplace=True)\n",
       "              (3): Conv2d(2, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (4): Sigmoid()\n",
       "            )\n",
       "            (sSE): Sequential(\n",
       "              (0): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): SCSEModule(\n",
       "            (cSE): Sequential(\n",
       "              (0): AdaptiveAvgPool2d(output_size=1)\n",
       "              (1): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (2): ReLU(inplace=True)\n",
       "              (3): Conv2d(1, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (4): Sigmoid()\n",
       "            )\n",
       "            (sSE): Sequential(\n",
       "              (0): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): Sigmoid()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (segmentation_head): SegmentationHead(\n",
       "    (0): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Identity()\n",
       "    (2): Activation(\n",
       "      (activation): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = smp.Unet(\n",
    "    encoder_name=Params.encoder_name,\n",
    "    encoder_weights=Params.encoder_weights,\n",
    "    in_channels=Params.channels,\n",
    "    decoder_attention_type=Params.decoder_attention_type,\n",
    "    classes=Params.classes,\n",
    "    activation=Params.activation_layer,\n",
    ")\n",
    "\n",
    "# model = torch.compile(model, mode=Params.compile_mode)\n",
    "model.to(Params.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [128, 64, 16, 16]           3,136\n",
      "       BatchNorm2d-2          [128, 64, 16, 16]             128\n",
      "              ReLU-3          [128, 64, 16, 16]               0\n",
      "         MaxPool2d-4            [128, 64, 8, 8]               0\n",
      "            Conv2d-5            [128, 64, 8, 8]           4,096\n",
      "       BatchNorm2d-6            [128, 64, 8, 8]             128\n",
      "              ReLU-7            [128, 64, 8, 8]               0\n",
      "            Conv2d-8            [128, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9            [128, 64, 8, 8]             128\n",
      "             ReLU-10            [128, 64, 8, 8]               0\n",
      "           Conv2d-11           [128, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-12           [128, 256, 8, 8]             512\n",
      "           Conv2d-13           [128, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-14           [128, 256, 8, 8]             512\n",
      "             ReLU-15           [128, 256, 8, 8]               0\n",
      "       Bottleneck-16           [128, 256, 8, 8]               0\n",
      "           Conv2d-17            [128, 64, 8, 8]          16,384\n",
      "      BatchNorm2d-18            [128, 64, 8, 8]             128\n",
      "             ReLU-19            [128, 64, 8, 8]               0\n",
      "           Conv2d-20            [128, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-21            [128, 64, 8, 8]             128\n",
      "             ReLU-22            [128, 64, 8, 8]               0\n",
      "           Conv2d-23           [128, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-24           [128, 256, 8, 8]             512\n",
      "             ReLU-25           [128, 256, 8, 8]               0\n",
      "       Bottleneck-26           [128, 256, 8, 8]               0\n",
      "           Conv2d-27            [128, 64, 8, 8]          16,384\n",
      "      BatchNorm2d-28            [128, 64, 8, 8]             128\n",
      "             ReLU-29            [128, 64, 8, 8]               0\n",
      "           Conv2d-30            [128, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-31            [128, 64, 8, 8]             128\n",
      "             ReLU-32            [128, 64, 8, 8]               0\n",
      "           Conv2d-33           [128, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-34           [128, 256, 8, 8]             512\n",
      "             ReLU-35           [128, 256, 8, 8]               0\n",
      "       Bottleneck-36           [128, 256, 8, 8]               0\n",
      "           Conv2d-37           [128, 128, 8, 8]          32,768\n",
      "      BatchNorm2d-38           [128, 128, 8, 8]             256\n",
      "             ReLU-39           [128, 128, 8, 8]               0\n",
      "           Conv2d-40           [128, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-41           [128, 128, 4, 4]             256\n",
      "             ReLU-42           [128, 128, 4, 4]               0\n",
      "           Conv2d-43           [128, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-44           [128, 512, 4, 4]           1,024\n",
      "           Conv2d-45           [128, 512, 4, 4]         131,072\n",
      "      BatchNorm2d-46           [128, 512, 4, 4]           1,024\n",
      "             ReLU-47           [128, 512, 4, 4]               0\n",
      "       Bottleneck-48           [128, 512, 4, 4]               0\n",
      "           Conv2d-49           [128, 128, 4, 4]          65,536\n",
      "      BatchNorm2d-50           [128, 128, 4, 4]             256\n",
      "             ReLU-51           [128, 128, 4, 4]               0\n",
      "           Conv2d-52           [128, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-53           [128, 128, 4, 4]             256\n",
      "             ReLU-54           [128, 128, 4, 4]               0\n",
      "           Conv2d-55           [128, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-56           [128, 512, 4, 4]           1,024\n",
      "             ReLU-57           [128, 512, 4, 4]               0\n",
      "       Bottleneck-58           [128, 512, 4, 4]               0\n",
      "           Conv2d-59           [128, 128, 4, 4]          65,536\n",
      "      BatchNorm2d-60           [128, 128, 4, 4]             256\n",
      "             ReLU-61           [128, 128, 4, 4]               0\n",
      "           Conv2d-62           [128, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-63           [128, 128, 4, 4]             256\n",
      "             ReLU-64           [128, 128, 4, 4]               0\n",
      "           Conv2d-65           [128, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-66           [128, 512, 4, 4]           1,024\n",
      "             ReLU-67           [128, 512, 4, 4]               0\n",
      "       Bottleneck-68           [128, 512, 4, 4]               0\n",
      "           Conv2d-69           [128, 128, 4, 4]          65,536\n",
      "      BatchNorm2d-70           [128, 128, 4, 4]             256\n",
      "             ReLU-71           [128, 128, 4, 4]               0\n",
      "           Conv2d-72           [128, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-73           [128, 128, 4, 4]             256\n",
      "             ReLU-74           [128, 128, 4, 4]               0\n",
      "           Conv2d-75           [128, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-76           [128, 512, 4, 4]           1,024\n",
      "             ReLU-77           [128, 512, 4, 4]               0\n",
      "       Bottleneck-78           [128, 512, 4, 4]               0\n",
      "           Conv2d-79           [128, 128, 4, 4]          65,536\n",
      "      BatchNorm2d-80           [128, 128, 4, 4]             256\n",
      "             ReLU-81           [128, 128, 4, 4]               0\n",
      "           Conv2d-82           [128, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-83           [128, 128, 4, 4]             256\n",
      "             ReLU-84           [128, 128, 4, 4]               0\n",
      "           Conv2d-85           [128, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-86           [128, 512, 4, 4]           1,024\n",
      "             ReLU-87           [128, 512, 4, 4]               0\n",
      "       Bottleneck-88           [128, 512, 4, 4]               0\n",
      "           Conv2d-89           [128, 128, 4, 4]          65,536\n",
      "      BatchNorm2d-90           [128, 128, 4, 4]             256\n",
      "             ReLU-91           [128, 128, 4, 4]               0\n",
      "           Conv2d-92           [128, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-93           [128, 128, 4, 4]             256\n",
      "             ReLU-94           [128, 128, 4, 4]               0\n",
      "           Conv2d-95           [128, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-96           [128, 512, 4, 4]           1,024\n",
      "             ReLU-97           [128, 512, 4, 4]               0\n",
      "       Bottleneck-98           [128, 512, 4, 4]               0\n",
      "           Conv2d-99           [128, 128, 4, 4]          65,536\n",
      "     BatchNorm2d-100           [128, 128, 4, 4]             256\n",
      "            ReLU-101           [128, 128, 4, 4]               0\n",
      "          Conv2d-102           [128, 128, 4, 4]         147,456\n",
      "     BatchNorm2d-103           [128, 128, 4, 4]             256\n",
      "            ReLU-104           [128, 128, 4, 4]               0\n",
      "          Conv2d-105           [128, 512, 4, 4]          65,536\n",
      "     BatchNorm2d-106           [128, 512, 4, 4]           1,024\n",
      "            ReLU-107           [128, 512, 4, 4]               0\n",
      "      Bottleneck-108           [128, 512, 4, 4]               0\n",
      "          Conv2d-109           [128, 128, 4, 4]          65,536\n",
      "     BatchNorm2d-110           [128, 128, 4, 4]             256\n",
      "            ReLU-111           [128, 128, 4, 4]               0\n",
      "          Conv2d-112           [128, 128, 4, 4]         147,456\n",
      "     BatchNorm2d-113           [128, 128, 4, 4]             256\n",
      "            ReLU-114           [128, 128, 4, 4]               0\n",
      "          Conv2d-115           [128, 512, 4, 4]          65,536\n",
      "     BatchNorm2d-116           [128, 512, 4, 4]           1,024\n",
      "            ReLU-117           [128, 512, 4, 4]               0\n",
      "      Bottleneck-118           [128, 512, 4, 4]               0\n",
      "          Conv2d-119           [128, 256, 4, 4]         131,072\n",
      "     BatchNorm2d-120           [128, 256, 4, 4]             512\n",
      "            ReLU-121           [128, 256, 4, 4]               0\n",
      "          Conv2d-122           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-123           [128, 256, 2, 2]             512\n",
      "            ReLU-124           [128, 256, 2, 2]               0\n",
      "          Conv2d-125          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-126          [128, 1024, 2, 2]           2,048\n",
      "          Conv2d-127          [128, 1024, 2, 2]         524,288\n",
      "     BatchNorm2d-128          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-129          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-130          [128, 1024, 2, 2]               0\n",
      "          Conv2d-131           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-132           [128, 256, 2, 2]             512\n",
      "            ReLU-133           [128, 256, 2, 2]               0\n",
      "          Conv2d-134           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-135           [128, 256, 2, 2]             512\n",
      "            ReLU-136           [128, 256, 2, 2]               0\n",
      "          Conv2d-137          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-138          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-139          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-140          [128, 1024, 2, 2]               0\n",
      "          Conv2d-141           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-142           [128, 256, 2, 2]             512\n",
      "            ReLU-143           [128, 256, 2, 2]               0\n",
      "          Conv2d-144           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-145           [128, 256, 2, 2]             512\n",
      "            ReLU-146           [128, 256, 2, 2]               0\n",
      "          Conv2d-147          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-148          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-149          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-150          [128, 1024, 2, 2]               0\n",
      "          Conv2d-151           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-152           [128, 256, 2, 2]             512\n",
      "            ReLU-153           [128, 256, 2, 2]               0\n",
      "          Conv2d-154           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-155           [128, 256, 2, 2]             512\n",
      "            ReLU-156           [128, 256, 2, 2]               0\n",
      "          Conv2d-157          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-158          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-159          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-160          [128, 1024, 2, 2]               0\n",
      "          Conv2d-161           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-162           [128, 256, 2, 2]             512\n",
      "            ReLU-163           [128, 256, 2, 2]               0\n",
      "          Conv2d-164           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-165           [128, 256, 2, 2]             512\n",
      "            ReLU-166           [128, 256, 2, 2]               0\n",
      "          Conv2d-167          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-168          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-169          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-170          [128, 1024, 2, 2]               0\n",
      "          Conv2d-171           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-172           [128, 256, 2, 2]             512\n",
      "            ReLU-173           [128, 256, 2, 2]               0\n",
      "          Conv2d-174           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-175           [128, 256, 2, 2]             512\n",
      "            ReLU-176           [128, 256, 2, 2]               0\n",
      "          Conv2d-177          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-178          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-179          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-180          [128, 1024, 2, 2]               0\n",
      "          Conv2d-181           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-182           [128, 256, 2, 2]             512\n",
      "            ReLU-183           [128, 256, 2, 2]               0\n",
      "          Conv2d-184           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-185           [128, 256, 2, 2]             512\n",
      "            ReLU-186           [128, 256, 2, 2]               0\n",
      "          Conv2d-187          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-188          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-189          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-190          [128, 1024, 2, 2]               0\n",
      "          Conv2d-191           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-192           [128, 256, 2, 2]             512\n",
      "            ReLU-193           [128, 256, 2, 2]               0\n",
      "          Conv2d-194           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-195           [128, 256, 2, 2]             512\n",
      "            ReLU-196           [128, 256, 2, 2]               0\n",
      "          Conv2d-197          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-198          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-199          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-200          [128, 1024, 2, 2]               0\n",
      "          Conv2d-201           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-202           [128, 256, 2, 2]             512\n",
      "            ReLU-203           [128, 256, 2, 2]               0\n",
      "          Conv2d-204           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-205           [128, 256, 2, 2]             512\n",
      "            ReLU-206           [128, 256, 2, 2]               0\n",
      "          Conv2d-207          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-208          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-209          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-210          [128, 1024, 2, 2]               0\n",
      "          Conv2d-211           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-212           [128, 256, 2, 2]             512\n",
      "            ReLU-213           [128, 256, 2, 2]               0\n",
      "          Conv2d-214           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-215           [128, 256, 2, 2]             512\n",
      "            ReLU-216           [128, 256, 2, 2]               0\n",
      "          Conv2d-217          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-218          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-219          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-220          [128, 1024, 2, 2]               0\n",
      "          Conv2d-221           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-222           [128, 256, 2, 2]             512\n",
      "            ReLU-223           [128, 256, 2, 2]               0\n",
      "          Conv2d-224           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-225           [128, 256, 2, 2]             512\n",
      "            ReLU-226           [128, 256, 2, 2]               0\n",
      "          Conv2d-227          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-228          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-229          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-230          [128, 1024, 2, 2]               0\n",
      "          Conv2d-231           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-232           [128, 256, 2, 2]             512\n",
      "            ReLU-233           [128, 256, 2, 2]               0\n",
      "          Conv2d-234           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-235           [128, 256, 2, 2]             512\n",
      "            ReLU-236           [128, 256, 2, 2]               0\n",
      "          Conv2d-237          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-238          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-239          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-240          [128, 1024, 2, 2]               0\n",
      "          Conv2d-241           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-242           [128, 256, 2, 2]             512\n",
      "            ReLU-243           [128, 256, 2, 2]               0\n",
      "          Conv2d-244           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-245           [128, 256, 2, 2]             512\n",
      "            ReLU-246           [128, 256, 2, 2]               0\n",
      "          Conv2d-247          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-248          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-249          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-250          [128, 1024, 2, 2]               0\n",
      "          Conv2d-251           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-252           [128, 256, 2, 2]             512\n",
      "            ReLU-253           [128, 256, 2, 2]               0\n",
      "          Conv2d-254           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-255           [128, 256, 2, 2]             512\n",
      "            ReLU-256           [128, 256, 2, 2]               0\n",
      "          Conv2d-257          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-258          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-259          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-260          [128, 1024, 2, 2]               0\n",
      "          Conv2d-261           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-262           [128, 256, 2, 2]             512\n",
      "            ReLU-263           [128, 256, 2, 2]               0\n",
      "          Conv2d-264           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-265           [128, 256, 2, 2]             512\n",
      "            ReLU-266           [128, 256, 2, 2]               0\n",
      "          Conv2d-267          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-268          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-269          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-270          [128, 1024, 2, 2]               0\n",
      "          Conv2d-271           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-272           [128, 256, 2, 2]             512\n",
      "            ReLU-273           [128, 256, 2, 2]               0\n",
      "          Conv2d-274           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-275           [128, 256, 2, 2]             512\n",
      "            ReLU-276           [128, 256, 2, 2]               0\n",
      "          Conv2d-277          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-278          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-279          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-280          [128, 1024, 2, 2]               0\n",
      "          Conv2d-281           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-282           [128, 256, 2, 2]             512\n",
      "            ReLU-283           [128, 256, 2, 2]               0\n",
      "          Conv2d-284           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-285           [128, 256, 2, 2]             512\n",
      "            ReLU-286           [128, 256, 2, 2]               0\n",
      "          Conv2d-287          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-288          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-289          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-290          [128, 1024, 2, 2]               0\n",
      "          Conv2d-291           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-292           [128, 256, 2, 2]             512\n",
      "            ReLU-293           [128, 256, 2, 2]               0\n",
      "          Conv2d-294           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-295           [128, 256, 2, 2]             512\n",
      "            ReLU-296           [128, 256, 2, 2]               0\n",
      "          Conv2d-297          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-298          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-299          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-300          [128, 1024, 2, 2]               0\n",
      "          Conv2d-301           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-302           [128, 256, 2, 2]             512\n",
      "            ReLU-303           [128, 256, 2, 2]               0\n",
      "          Conv2d-304           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-305           [128, 256, 2, 2]             512\n",
      "            ReLU-306           [128, 256, 2, 2]               0\n",
      "          Conv2d-307          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-308          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-309          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-310          [128, 1024, 2, 2]               0\n",
      "          Conv2d-311           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-312           [128, 256, 2, 2]             512\n",
      "            ReLU-313           [128, 256, 2, 2]               0\n",
      "          Conv2d-314           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-315           [128, 256, 2, 2]             512\n",
      "            ReLU-316           [128, 256, 2, 2]               0\n",
      "          Conv2d-317          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-318          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-319          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-320          [128, 1024, 2, 2]               0\n",
      "          Conv2d-321           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-322           [128, 256, 2, 2]             512\n",
      "            ReLU-323           [128, 256, 2, 2]               0\n",
      "          Conv2d-324           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-325           [128, 256, 2, 2]             512\n",
      "            ReLU-326           [128, 256, 2, 2]               0\n",
      "          Conv2d-327          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-328          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-329          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-330          [128, 1024, 2, 2]               0\n",
      "          Conv2d-331           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-332           [128, 256, 2, 2]             512\n",
      "            ReLU-333           [128, 256, 2, 2]               0\n",
      "          Conv2d-334           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-335           [128, 256, 2, 2]             512\n",
      "            ReLU-336           [128, 256, 2, 2]               0\n",
      "          Conv2d-337          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-338          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-339          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-340          [128, 1024, 2, 2]               0\n",
      "          Conv2d-341           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-342           [128, 256, 2, 2]             512\n",
      "            ReLU-343           [128, 256, 2, 2]               0\n",
      "          Conv2d-344           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-345           [128, 256, 2, 2]             512\n",
      "            ReLU-346           [128, 256, 2, 2]               0\n",
      "          Conv2d-347          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-348          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-349          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-350          [128, 1024, 2, 2]               0\n",
      "          Conv2d-351           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-352           [128, 256, 2, 2]             512\n",
      "            ReLU-353           [128, 256, 2, 2]               0\n",
      "          Conv2d-354           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-355           [128, 256, 2, 2]             512\n",
      "            ReLU-356           [128, 256, 2, 2]               0\n",
      "          Conv2d-357          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-358          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-359          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-360          [128, 1024, 2, 2]               0\n",
      "          Conv2d-361           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-362           [128, 256, 2, 2]             512\n",
      "            ReLU-363           [128, 256, 2, 2]               0\n",
      "          Conv2d-364           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-365           [128, 256, 2, 2]             512\n",
      "            ReLU-366           [128, 256, 2, 2]               0\n",
      "          Conv2d-367          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-368          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-369          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-370          [128, 1024, 2, 2]               0\n",
      "          Conv2d-371           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-372           [128, 256, 2, 2]             512\n",
      "            ReLU-373           [128, 256, 2, 2]               0\n",
      "          Conv2d-374           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-375           [128, 256, 2, 2]             512\n",
      "            ReLU-376           [128, 256, 2, 2]               0\n",
      "          Conv2d-377          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-378          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-379          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-380          [128, 1024, 2, 2]               0\n",
      "          Conv2d-381           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-382           [128, 256, 2, 2]             512\n",
      "            ReLU-383           [128, 256, 2, 2]               0\n",
      "          Conv2d-384           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-385           [128, 256, 2, 2]             512\n",
      "            ReLU-386           [128, 256, 2, 2]               0\n",
      "          Conv2d-387          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-388          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-389          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-390          [128, 1024, 2, 2]               0\n",
      "          Conv2d-391           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-392           [128, 256, 2, 2]             512\n",
      "            ReLU-393           [128, 256, 2, 2]               0\n",
      "          Conv2d-394           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-395           [128, 256, 2, 2]             512\n",
      "            ReLU-396           [128, 256, 2, 2]               0\n",
      "          Conv2d-397          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-398          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-399          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-400          [128, 1024, 2, 2]               0\n",
      "          Conv2d-401           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-402           [128, 256, 2, 2]             512\n",
      "            ReLU-403           [128, 256, 2, 2]               0\n",
      "          Conv2d-404           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-405           [128, 256, 2, 2]             512\n",
      "            ReLU-406           [128, 256, 2, 2]               0\n",
      "          Conv2d-407          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-408          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-409          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-410          [128, 1024, 2, 2]               0\n",
      "          Conv2d-411           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-412           [128, 256, 2, 2]             512\n",
      "            ReLU-413           [128, 256, 2, 2]               0\n",
      "          Conv2d-414           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-415           [128, 256, 2, 2]             512\n",
      "            ReLU-416           [128, 256, 2, 2]               0\n",
      "          Conv2d-417          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-418          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-419          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-420          [128, 1024, 2, 2]               0\n",
      "          Conv2d-421           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-422           [128, 256, 2, 2]             512\n",
      "            ReLU-423           [128, 256, 2, 2]               0\n",
      "          Conv2d-424           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-425           [128, 256, 2, 2]             512\n",
      "            ReLU-426           [128, 256, 2, 2]               0\n",
      "          Conv2d-427          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-428          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-429          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-430          [128, 1024, 2, 2]               0\n",
      "          Conv2d-431           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-432           [128, 256, 2, 2]             512\n",
      "            ReLU-433           [128, 256, 2, 2]               0\n",
      "          Conv2d-434           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-435           [128, 256, 2, 2]             512\n",
      "            ReLU-436           [128, 256, 2, 2]               0\n",
      "          Conv2d-437          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-438          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-439          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-440          [128, 1024, 2, 2]               0\n",
      "          Conv2d-441           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-442           [128, 256, 2, 2]             512\n",
      "            ReLU-443           [128, 256, 2, 2]               0\n",
      "          Conv2d-444           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-445           [128, 256, 2, 2]             512\n",
      "            ReLU-446           [128, 256, 2, 2]               0\n",
      "          Conv2d-447          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-448          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-449          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-450          [128, 1024, 2, 2]               0\n",
      "          Conv2d-451           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-452           [128, 256, 2, 2]             512\n",
      "            ReLU-453           [128, 256, 2, 2]               0\n",
      "          Conv2d-454           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-455           [128, 256, 2, 2]             512\n",
      "            ReLU-456           [128, 256, 2, 2]               0\n",
      "          Conv2d-457          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-458          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-459          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-460          [128, 1024, 2, 2]               0\n",
      "          Conv2d-461           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-462           [128, 256, 2, 2]             512\n",
      "            ReLU-463           [128, 256, 2, 2]               0\n",
      "          Conv2d-464           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-465           [128, 256, 2, 2]             512\n",
      "            ReLU-466           [128, 256, 2, 2]               0\n",
      "          Conv2d-467          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-468          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-469          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-470          [128, 1024, 2, 2]               0\n",
      "          Conv2d-471           [128, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-472           [128, 256, 2, 2]             512\n",
      "            ReLU-473           [128, 256, 2, 2]               0\n",
      "          Conv2d-474           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-475           [128, 256, 2, 2]             512\n",
      "            ReLU-476           [128, 256, 2, 2]               0\n",
      "          Conv2d-477          [128, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-478          [128, 1024, 2, 2]           2,048\n",
      "            ReLU-479          [128, 1024, 2, 2]               0\n",
      "      Bottleneck-480          [128, 1024, 2, 2]               0\n",
      "          Conv2d-481           [128, 512, 2, 2]         524,288\n",
      "     BatchNorm2d-482           [128, 512, 2, 2]           1,024\n",
      "            ReLU-483           [128, 512, 2, 2]               0\n",
      "          Conv2d-484           [128, 512, 1, 1]       2,359,296\n",
      "     BatchNorm2d-485           [128, 512, 1, 1]           1,024\n",
      "            ReLU-486           [128, 512, 1, 1]               0\n",
      "          Conv2d-487          [128, 2048, 1, 1]       1,048,576\n",
      "     BatchNorm2d-488          [128, 2048, 1, 1]           4,096\n",
      "          Conv2d-489          [128, 2048, 1, 1]       2,097,152\n",
      "     BatchNorm2d-490          [128, 2048, 1, 1]           4,096\n",
      "            ReLU-491          [128, 2048, 1, 1]               0\n",
      "      Bottleneck-492          [128, 2048, 1, 1]               0\n",
      "          Conv2d-493           [128, 512, 1, 1]       1,048,576\n",
      "     BatchNorm2d-494           [128, 512, 1, 1]           1,024\n",
      "            ReLU-495           [128, 512, 1, 1]               0\n",
      "          Conv2d-496           [128, 512, 1, 1]       2,359,296\n",
      "     BatchNorm2d-497           [128, 512, 1, 1]           1,024\n",
      "            ReLU-498           [128, 512, 1, 1]               0\n",
      "          Conv2d-499          [128, 2048, 1, 1]       1,048,576\n",
      "     BatchNorm2d-500          [128, 2048, 1, 1]           4,096\n",
      "            ReLU-501          [128, 2048, 1, 1]               0\n",
      "      Bottleneck-502          [128, 2048, 1, 1]               0\n",
      "          Conv2d-503           [128, 512, 1, 1]       1,048,576\n",
      "     BatchNorm2d-504           [128, 512, 1, 1]           1,024\n",
      "            ReLU-505           [128, 512, 1, 1]               0\n",
      "          Conv2d-506           [128, 512, 1, 1]       2,359,296\n",
      "     BatchNorm2d-507           [128, 512, 1, 1]           1,024\n",
      "            ReLU-508           [128, 512, 1, 1]               0\n",
      "          Conv2d-509          [128, 2048, 1, 1]       1,048,576\n",
      "     BatchNorm2d-510          [128, 2048, 1, 1]           4,096\n",
      "            ReLU-511          [128, 2048, 1, 1]               0\n",
      "      Bottleneck-512          [128, 2048, 1, 1]               0\n",
      "   ResNetEncoder-513  [[-1, 1, 32, 32], [-1, 64, 16, 16], [-1, 256, 8, 8], [-1, 512, 4, 4], [-1, 1024, 2, 2], [-1, 2048, 1, 1]]               0\n",
      "        Identity-514          [128, 2048, 1, 1]               0\n",
      "AdaptiveAvgPool2d-515          [128, 3072, 1, 1]               0\n",
      "          Conv2d-516           [128, 192, 1, 1]         590,016\n",
      "            ReLU-517           [128, 192, 1, 1]               0\n",
      "          Conv2d-518          [128, 3072, 1, 1]         592,896\n",
      "         Sigmoid-519          [128, 3072, 1, 1]               0\n",
      "          Conv2d-520             [128, 1, 2, 2]           3,073\n",
      "         Sigmoid-521             [128, 1, 2, 2]               0\n",
      "      SCSEModule-522          [128, 3072, 2, 2]               0\n",
      "       Attention-523          [128, 3072, 2, 2]               0\n",
      "          Conv2d-524           [128, 256, 2, 2]       7,077,888\n",
      "     BatchNorm2d-525           [128, 256, 2, 2]             512\n",
      "            ReLU-526           [128, 256, 2, 2]               0\n",
      "          Conv2d-527           [128, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-528           [128, 256, 2, 2]             512\n",
      "            ReLU-529           [128, 256, 2, 2]               0\n",
      "AdaptiveAvgPool2d-530           [128, 256, 1, 1]               0\n",
      "          Conv2d-531            [128, 16, 1, 1]           4,112\n",
      "            ReLU-532            [128, 16, 1, 1]               0\n",
      "          Conv2d-533           [128, 256, 1, 1]           4,352\n",
      "         Sigmoid-534           [128, 256, 1, 1]               0\n",
      "          Conv2d-535             [128, 1, 2, 2]             257\n",
      "         Sigmoid-536             [128, 1, 2, 2]               0\n",
      "      SCSEModule-537           [128, 256, 2, 2]               0\n",
      "       Attention-538           [128, 256, 2, 2]               0\n",
      "    DecoderBlock-539           [128, 256, 2, 2]               0\n",
      "AdaptiveAvgPool2d-540           [128, 768, 1, 1]               0\n",
      "          Conv2d-541            [128, 48, 1, 1]          36,912\n",
      "            ReLU-542            [128, 48, 1, 1]               0\n",
      "          Conv2d-543           [128, 768, 1, 1]          37,632\n",
      "         Sigmoid-544           [128, 768, 1, 1]               0\n",
      "          Conv2d-545             [128, 1, 4, 4]             769\n",
      "         Sigmoid-546             [128, 1, 4, 4]               0\n",
      "      SCSEModule-547           [128, 768, 4, 4]               0\n",
      "       Attention-548           [128, 768, 4, 4]               0\n",
      "          Conv2d-549           [128, 128, 4, 4]         884,736\n",
      "     BatchNorm2d-550           [128, 128, 4, 4]             256\n",
      "            ReLU-551           [128, 128, 4, 4]               0\n",
      "          Conv2d-552           [128, 128, 4, 4]         147,456\n",
      "     BatchNorm2d-553           [128, 128, 4, 4]             256\n",
      "            ReLU-554           [128, 128, 4, 4]               0\n",
      "AdaptiveAvgPool2d-555           [128, 128, 1, 1]               0\n",
      "          Conv2d-556             [128, 8, 1, 1]           1,032\n",
      "            ReLU-557             [128, 8, 1, 1]               0\n",
      "          Conv2d-558           [128, 128, 1, 1]           1,152\n",
      "         Sigmoid-559           [128, 128, 1, 1]               0\n",
      "          Conv2d-560             [128, 1, 4, 4]             129\n",
      "         Sigmoid-561             [128, 1, 4, 4]               0\n",
      "      SCSEModule-562           [128, 128, 4, 4]               0\n",
      "       Attention-563           [128, 128, 4, 4]               0\n",
      "    DecoderBlock-564           [128, 128, 4, 4]               0\n",
      "AdaptiveAvgPool2d-565           [128, 384, 1, 1]               0\n",
      "          Conv2d-566            [128, 24, 1, 1]           9,240\n",
      "            ReLU-567            [128, 24, 1, 1]               0\n",
      "          Conv2d-568           [128, 384, 1, 1]           9,600\n",
      "         Sigmoid-569           [128, 384, 1, 1]               0\n",
      "          Conv2d-570             [128, 1, 8, 8]             385\n",
      "         Sigmoid-571             [128, 1, 8, 8]               0\n",
      "      SCSEModule-572           [128, 384, 8, 8]               0\n",
      "       Attention-573           [128, 384, 8, 8]               0\n",
      "          Conv2d-574            [128, 64, 8, 8]         221,184\n",
      "     BatchNorm2d-575            [128, 64, 8, 8]             128\n",
      "            ReLU-576            [128, 64, 8, 8]               0\n",
      "          Conv2d-577            [128, 64, 8, 8]          36,864\n",
      "     BatchNorm2d-578            [128, 64, 8, 8]             128\n",
      "            ReLU-579            [128, 64, 8, 8]               0\n",
      "AdaptiveAvgPool2d-580            [128, 64, 1, 1]               0\n",
      "          Conv2d-581             [128, 4, 1, 1]             260\n",
      "            ReLU-582             [128, 4, 1, 1]               0\n",
      "          Conv2d-583            [128, 64, 1, 1]             320\n",
      "         Sigmoid-584            [128, 64, 1, 1]               0\n",
      "          Conv2d-585             [128, 1, 8, 8]              65\n",
      "         Sigmoid-586             [128, 1, 8, 8]               0\n",
      "      SCSEModule-587            [128, 64, 8, 8]               0\n",
      "       Attention-588            [128, 64, 8, 8]               0\n",
      "    DecoderBlock-589            [128, 64, 8, 8]               0\n",
      "AdaptiveAvgPool2d-590           [128, 128, 1, 1]               0\n",
      "          Conv2d-591             [128, 8, 1, 1]           1,032\n",
      "            ReLU-592             [128, 8, 1, 1]               0\n",
      "          Conv2d-593           [128, 128, 1, 1]           1,152\n",
      "         Sigmoid-594           [128, 128, 1, 1]               0\n",
      "          Conv2d-595           [128, 1, 16, 16]             129\n",
      "         Sigmoid-596           [128, 1, 16, 16]               0\n",
      "      SCSEModule-597         [128, 128, 16, 16]               0\n",
      "       Attention-598         [128, 128, 16, 16]               0\n",
      "          Conv2d-599          [128, 32, 16, 16]          36,864\n",
      "     BatchNorm2d-600          [128, 32, 16, 16]              64\n",
      "            ReLU-601          [128, 32, 16, 16]               0\n",
      "          Conv2d-602          [128, 32, 16, 16]           9,216\n",
      "     BatchNorm2d-603          [128, 32, 16, 16]              64\n",
      "            ReLU-604          [128, 32, 16, 16]               0\n",
      "AdaptiveAvgPool2d-605            [128, 32, 1, 1]               0\n",
      "          Conv2d-606             [128, 2, 1, 1]              66\n",
      "            ReLU-607             [128, 2, 1, 1]               0\n",
      "          Conv2d-608            [128, 32, 1, 1]              96\n",
      "         Sigmoid-609            [128, 32, 1, 1]               0\n",
      "          Conv2d-610           [128, 1, 16, 16]              33\n",
      "         Sigmoid-611           [128, 1, 16, 16]               0\n",
      "      SCSEModule-612          [128, 32, 16, 16]               0\n",
      "       Attention-613          [128, 32, 16, 16]               0\n",
      "    DecoderBlock-614          [128, 32, 16, 16]               0\n",
      "          Conv2d-615          [128, 16, 32, 32]           4,608\n",
      "     BatchNorm2d-616          [128, 16, 32, 32]              32\n",
      "            ReLU-617          [128, 16, 32, 32]               0\n",
      "          Conv2d-618          [128, 16, 32, 32]           2,304\n",
      "     BatchNorm2d-619          [128, 16, 32, 32]              32\n",
      "            ReLU-620          [128, 16, 32, 32]               0\n",
      "AdaptiveAvgPool2d-621            [128, 16, 1, 1]               0\n",
      "          Conv2d-622             [128, 1, 1, 1]              17\n",
      "            ReLU-623             [128, 1, 1, 1]               0\n",
      "          Conv2d-624            [128, 16, 1, 1]              32\n",
      "         Sigmoid-625            [128, 16, 1, 1]               0\n",
      "          Conv2d-626           [128, 1, 32, 32]              17\n",
      "         Sigmoid-627           [128, 1, 32, 32]               0\n",
      "      SCSEModule-628          [128, 16, 32, 32]               0\n",
      "       Attention-629          [128, 16, 32, 32]               0\n",
      "    DecoderBlock-630          [128, 16, 32, 32]               0\n",
      "     UnetDecoder-631          [128, 16, 32, 32]               0\n",
      "          Conv2d-632           [128, 1, 32, 32]             145\n",
      "        Identity-633           [128, 1, 32, 32]               0\n",
      "        Identity-634           [128, 1, 32, 32]               0\n",
      "      Activation-635           [128, 1, 32, 32]               0\n",
      "================================================================\n",
      "Total params: 68,445,385\n",
      "Trainable params: 68,445,385\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.50\n",
      "Forward/backward pass size (MB): 2063.62\n",
      "Params size (MB): 261.10\n",
      "Estimated Total Size (MB): 2325.22\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print model summary\n",
    "summary(\n",
    "    model,\n",
    "    (Params.channels, *Params.image_reshape),\n",
    "    batch_size=Params.batch_size,\n",
    "    device=Params.device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss(reduction=\"none\").to(Params.device)\n",
    "scaler = torch.GradScaler(device=Params.device)\n",
    "optimizer = optim.Adam(\n",
    "    params=model.parameters(),\n",
    "    lr=Params.learning_rate,\n",
    "    amsgrad=Params.true_boolean,\n",
    "    weight_decay=Params.learning_rate * 0.1,\n",
    "    foreach=Params.true_boolean,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler = optim.lr_scheduler.StepLR(\n",
    "#     optimizer=optimizer, step_size=10, gamma=0.1\n",
    "# )  # Reduce LR by a factor of 10 every 10 epochs\n",
    "\n",
    "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"min\",\n",
    "    factor=0.1,\n",
    "    patience=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/contepablod/UNet/e/UN-125\n"
     ]
    }
   ],
   "source": [
    "run = neptune.init_run(\n",
    "    project=\"contepablod/UNet\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI2YjAzNmM1Zi0yYzM2LTQ1YTUtYjNmYi0xOGQ0NDgwYjY1NzMifQ==\",\n",
    "    capture_hardware_metrics=True,\n",
    "    capture_stderr=True,\n",
    "    capture_stdout=True,\n",
    "    capture_traceback=True,\n",
    ")  # Start Neptune Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, loader, optimizer, criterion, mode, device, default_value):\n",
    "    if mode == \"train\":\n",
    "        model.train()\n",
    "        is_train = True\n",
    "    else:\n",
    "        model.eval()\n",
    "        is_train = False\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    nan_count = {\"input_images\": 0, \"input_labels\": 0, \"outputs\": 0, \"loss\": 0}\n",
    "    inf_count = {\"input_images\": 0, \"input_labels\": 0, \"outputs\": 0, \"loss\": 0}\n",
    "\n",
    "    for img_name, images, labels in tqdm(loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Check for NaNs and Infs in images and labels\n",
    "        if torch.isnan(images).any():\n",
    "            print(f\"NaN detected in {mode} input images\")\n",
    "            nan_count[\"input_images\"] += 1\n",
    "            continue\n",
    "        if torch.isnan(labels).any():\n",
    "            print(f\"NaN detected in {mode} input labels\")\n",
    "            nan_count[\"input_labels\"] += 1\n",
    "            continue\n",
    "        if torch.isinf(images).any():\n",
    "            print(f\"Inf detected in {mode} input images\")\n",
    "            inf_count[\"input_images\"] += 1\n",
    "            continue\n",
    "        if torch.isinf(labels).any():\n",
    "            print(f\"Inf detected in {mode} input labels\")\n",
    "            inf_count[\"input_labels\"] += 1\n",
    "            continue\n",
    "\n",
    "        # Reset gradients only for training\n",
    "        if is_train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.autocast(device_type=device):\n",
    "            mask = torch.ones_like(\n",
    "                images, dtype=torch.bool\n",
    "            )  # Initialize a mask of all True values\n",
    "\n",
    "            # Iterate over default values to update the mask\n",
    "            for default_value in default_values:\n",
    "                mask &= images != default_value\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "        # Check for NaNs and Infs in outputs\n",
    "        if torch.isnan(outputs).any():\n",
    "            print(f\"NaN detected in {mode} outputs\")\n",
    "            nan_count[\"outputs\"] += 1\n",
    "            continue\n",
    "        if torch.isinf(outputs).any():\n",
    "            print(f\"Inf detected in {mode} outputs\")\n",
    "            inf_count[\"outputs\"] += 1\n",
    "            continue\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        masked_loss = loss * mask\n",
    "\n",
    "        if torch.isnan(masked_loss).any():\n",
    "            print(f\"NaN detected in {mode} loss\")\n",
    "            nan_count[\"loss\"] += 1\n",
    "            continue\n",
    "        if torch.isinf(masked_loss).any():\n",
    "            print(f\"Inf detected in {mode} loss\")\n",
    "            inf_count[\"loss\"] += 1\n",
    "            continue\n",
    "\n",
    "        # Normalize the loss by valid points\n",
    "        valid_points = mask.sum()\n",
    "        if valid_points > 0:\n",
    "            masked_loss = masked_loss.sum() / valid_points\n",
    "        else:\n",
    "            print(f\"No valid points found in {mode}\")\n",
    "            continue\n",
    "\n",
    "        # Backpropagation only for training\n",
    "        if is_train:\n",
    "            scaler.scale(masked_loss).backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=Params.max_grad_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        running_loss += masked_loss.item()\n",
    "\n",
    "        if is_train:\n",
    "            run[\"train/loss\"].append(masked_loss.item())\n",
    "        else:\n",
    "            run[\"val/loss\"].append(masked_loss.item())\n",
    "\n",
    "        # Calculate predictions and accuracy\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).int()\n",
    "        masked_preds = preds[mask]\n",
    "        masked_labels = labels[mask]\n",
    "        correct = (masked_preds == masked_labels).sum().item()\n",
    "        total = mask.sum().item()\n",
    "\n",
    "    # Calculate average loss and accuracy\n",
    "    avg_loss = running_loss / len(loader)\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "    return avg_loss, accuracy, nan_count, inf_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    scheduler,\n",
    "    device,\n",
    "    default_value,\n",
    "    num_epochs,\n",
    "):\n",
    "    # Initialize lists to store metrics\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    train_nan_count, train_inf_count = {}, {}\n",
    "    val_nan_count, val_inf_count = {}, {}\n",
    "\n",
    "    # Initialize plot\n",
    "    plt.ion()\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Run training epoch\n",
    "        train_loss, train_accuracy, train_nan_count, train_inf_count = run_epoch(\n",
    "            model,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            \"train\",\n",
    "            device,\n",
    "            default_value,\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        run[\"train/accuracy\"].append(train_accuracy)\n",
    "\n",
    "        # Run validation epoch\n",
    "        val_loss, val_accuracy, val_nan_count, val_inf_count = run_epoch(\n",
    "            model,\n",
    "            val_loader,\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            \"val\",\n",
    "            device,\n",
    "            default_value,\n",
    "        )\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        run[\"val/accuracy\"].append(val_accuracy)\n",
    "\n",
    "        # Update learning rate scheduler\n",
    "        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(val_loss)\n",
    "        else:\n",
    "            scheduler.step()  # Step for other schedulers\n",
    "\n",
    "        # Update the plot\n",
    "        ax1.clear()\n",
    "        ax2.clear()\n",
    "        ax1.plot(train_losses, label=\"Train Loss\")\n",
    "        ax1.plot(val_losses, label=\"Val Loss\")\n",
    "        ax1.set_title(\"Loss\")\n",
    "        ax1.legend()\n",
    "\n",
    "        ax2.plot(train_accuracies, label=\"Train Accuracy\")\n",
    "        ax2.plot(val_accuracies, label=\"Val Accuracy\")\n",
    "        ax2.set_title(\"Accuracy\")\n",
    "        ax2.legend()\n",
    "\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}]: \\\n",
    "            Train Loss: {train_loss:.4f} \\\n",
    "            Val Loss: {val_loss:.4f} \\\n",
    "            Train Acc: {train_accuracy:.4f} \\\n",
    "            Val Acc: {val_accuracy:.4f}\",\n",
    "            f\"\\nTrain Nan Count: {train_nan_count}, \\\n",
    "            Val Nan Count: {val_nan_count}, \\\n",
    "            Train Inf Count: {train_inf_count}, \\\n",
    "            Val Inf Count: {val_inf_count}\",\n",
    "        )\n",
    "        display(fig)\n",
    "\n",
    "    run.stop()  # Stop Neptune run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [02:15<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN detected in train outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [00:01<00:16,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN detected in val outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [00:02<00:15,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN detected in val outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [00:03<00:14,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN detected in val outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4/15 [00:04<00:13,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN detected in val outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5/15 [00:06<00:12,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN detected in val outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [00:07<00:11,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN detected in val outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7/15 [00:08<00:09,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN detected in val outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7/15 [00:08<00:10,  1.27s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mParams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mParams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 38\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, scheduler, device, default_value, num_epochs)\u001b[0m\n\u001b[1;32m     35\u001b[0m run[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain/accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_accuracy)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Run validation epoch\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m val_loss, val_accuracy, val_nan_count, val_inf_count \u001b[38;5;241m=\u001b[39m \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m val_losses\u001b[38;5;241m.\u001b[39mappend(val_loss)\n\u001b[1;32m     48\u001b[0m val_accuracies\u001b[38;5;241m.\u001b[39mappend(val_accuracy)\n",
      "Cell \u001b[0;32mIn[19], line 49\u001b[0m, in \u001b[0;36mrun_epoch\u001b[0;34m(model, loader, optimizer, criterion, mode, device, default_value)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m default_value \u001b[38;5;129;01min\u001b[39;00m default_values:\n\u001b[1;32m     47\u001b[0m         mask \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m images \u001b[38;5;241m!=\u001b[39m default_value\n\u001b[0;32m---> 49\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Check for NaNs and Infs in outputs\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(outputs)\u001b[38;5;241m.\u001b[39many():\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/segmentation_models_pytorch/base/model.py:38\u001b[0m, in \u001b[0;36mSegmentationModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_input_shape(x)\n\u001b[0;32m---> 38\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\u001b[38;5;241m*\u001b[39mfeatures)\n\u001b[1;32m     41\u001b[0m masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegmentation_head(decoder_output)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/segmentation_models_pytorch/encoders/resnet.py:63\u001b[0m, in \u001b[0;36mResNetEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m features \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 63\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mstages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torchvision/models/resnet.py:154\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    151\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n\u001b[1;32m    152\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m--> 154\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(out)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_evaluate(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    scheduler=scheduler,\n",
    "    device=Params.device,\n",
    "    default_value=default_values,\n",
    "    num_epochs=Params.num_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_pretrained.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = smp.Unet(\n",
    "        encoder_name=Params.encoder_name,\n",
    "        encoder_weights=Params.encoder_weights,\n",
    "        decoder_attention_type=Params.decoder_attention_type,\n",
    "        in_channels=Params.channels,\n",
    "        classes=Params.classes,\n",
    "        activation=Params.activation_layer,\n",
    "    )\n",
    "# model = torch.compile(model, mode=Params.compile_mode)\n",
    "model_test.load_state_dict(torch.load(\"model_pretrained.pt\", weights_only=True))\n",
    "model_test.to(Params.device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_score(preds, labels):\n",
    "    intersection = (preds * labels).sum()\n",
    "    union = preds.sum() + labels.sum() - intersection\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_train_scores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _, images, labels in train_loader:\n",
    "        images, labels = images.to(Params.device), labels.to(Params.device)\n",
    "        with torch.autocast(device_type=Params.device):\n",
    "            mask = torch.ones_like(images, dtype=torch.bool)\n",
    "            for default_value in default_values:\n",
    "                mask &= images != default_value\n",
    "            outputs = model(images)\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).int()\n",
    "        masked_preds = preds[mask]\n",
    "        masked_labels = labels[mask]\n",
    "        iou = iou_score(masked_preds, masked_labels)\n",
    "        iou_train_scores.append(iou.item())\n",
    "\n",
    "print(f\"Train Mean IoU: {np.mean(iou_train_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_val_scores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _, images, labels in val_loader:\n",
    "        images, labels = images.to(Params.device), labels.to(Params.device)\n",
    "        with torch.autocast(device_type=Params.device):\n",
    "            mask = torch.ones_like(images, dtype=torch.bool)\n",
    "            for default_value in default_values:\n",
    "                mask &= images != default_value\n",
    "            outputs = model(images)\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).int()\n",
    "        masked_preds = preds[mask]\n",
    "        masked_labels = labels[mask]\n",
    "        iou = iou_score(masked_preds, masked_labels)\n",
    "        iou_val_scores.append(iou.item())\n",
    "\n",
    "print(f\"Validation Mean IoU: {np.mean(iou_val_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestWellDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.images_dir = Params.test_images_dir\n",
    "        self.image_files = list(sorted(os.listdir(self.images_dir)))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        image = np.load(f\"{self.images_dir}/{img_name}\")\n",
    "        image = np.nan_to_num(image)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(\n",
    "                torch.tensor(image, dtype=torch.float32).unsqueeze(0)\n",
    "            )\n",
    "\n",
    "        # Remove the .npy extension from the image name\n",
    "        img_name = img_name.replace(\".npy\", \"\")\n",
    "\n",
    "        return img_name, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the test dataset and dataloader\n",
    "test_dataset = TestWellDataset(transform=transform)\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=Params.batch_size,\n",
    "    shuffle=Params.false_boolean,\n",
    "    num_workers=Params.num_workers,\n",
    "    pin_memory=Params.true_boolean,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_value_counter = Counter()\n",
    "\n",
    "for _, images in tqdm(test_loader):\n",
    "    flattened_images = images.cpu().numpy().flatten()\n",
    "    test_value_counter.update(flattened_images)\n",
    "\n",
    "test_most_common_values = train_value_counter.most_common(20)\n",
    "\n",
    "print(\"Most common pixel values in training:\", test_most_common_values)\n",
    "\n",
    "test_default_values = [\n",
    "    float(val) for val, count in test_most_common_values if is_default_value(val)\n",
    "]\n",
    "\n",
    "default_values = list(set(default_values + test_default_values))\n",
    "\n",
    "print(\"Default values detected:\", default_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import zoom\n",
    "\n",
    "predictions = []\n",
    "zoom_factors = (1, 36 / 32, 36 / 32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img_name, images in tqdm(test_loader):\n",
    "        images = images.to(Params.device)\n",
    "        with torch.autocast(device_type=Params.device):\n",
    "            masks = torch.ones_like(images, dtype=torch.bool)\n",
    "            for default_value in default_values:\n",
    "                masks &= images != default_value\n",
    "            outputs = model_test(images)\n",
    "            pred = (torch.sigmoid(outputs) > 0.5).int()\n",
    "            # # Iterate over batch\n",
    "            for i, img_name in enumerate(img_name):\n",
    "            #     # Resize the output (logits) and mask\n",
    "            #     # output_resized = zoom(\n",
    "            #     #     outputs[i].cpu().detach().numpy().astype(np.float32),\n",
    "            #     #     zoom_factors,\n",
    "            #     #     order=1,\n",
    "            #     # )\n",
    "            #     # mask_resized = zoom(\n",
    "            #     #     masks[i].cpu().detach().numpy().astype(np.float32),\n",
    "            #     #     zoom_factors,\n",
    "            #     #     order=1,\n",
    "            #     # )\n",
    "            # Apply sigmoid to get probabilities\n",
    "            # pred = (torch.sigmoid(torch.tensor(outputs)) > 0.5).int()\n",
    "            # pred = (torch.sigmoid(outputs) > 0.5).int()\n",
    "\n",
    "                # Apply the resized mask to zero out predictions corresponding to default value pixels\n",
    "                # mask_pred = pred[masks]\n",
    "                # print(mask_pred.shape)\n",
    "\n",
    "                # Flatten the predictions and save them\n",
    "                pred_flattened = pred[i].flatten()\n",
    "                predictions.append([img_name] + pred_flattened.tolist())\n",
    "\n",
    "# Create column names for the submission file\n",
    "column_names = [\"\"] + [f\"{i}\" for i in range(32 * 32)]\n",
    "\n",
    "# Create a DataFrame and save it to a CSV file in the required submission format\n",
    "submission_df = pd.DataFrame(predictions, columns=column_names)\n",
    "submission_df.to_csv(\"./submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
